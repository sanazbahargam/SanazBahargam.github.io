---
title: 'Conditional Random Field'
date: 2020-04-28
permalink: /posts/2020/04/CRF/
tags:
  - Conditional Random Field
  - CRF
  - Sequence Labeling
  - NLP
---

In this post, I briefly explain what is conditional random Fields and how they can be used for sequence labeling.
CRF is a discriminative model best suited for tasks in which contextual information or state of the neighbors affects the current prediction. CRFs are widely used in named entity recognition, part of speech tagging, gene prediction, noise reduction, and object detection problems. 

In order to understand this post, it’s helpful to first read about Hidden Markov Models and Gibbs Distribution. 
This video explains the basics of [HMM](https://www.youtube.com/watch?v=kqSzLo9fenk&ab_channel=LuisSerrano) and [this video](https://www.youtube.com/watch?v=IzlYOX0wrz0&ab_channel=MachineLearningTV) by Daphne Koller is a very good introduction on Gibbs distribution.

Now let’ see why we need CRF?
![pic](https://github.com/sanazbahargam/NAACL-2018/blob/master/images/NER_CRF.png?raw=true)

From the [paper](https://www.aclweb.org/anthology/W17-4421.pdf)


NER is in general formulated as a sequence labeling problem with a multi-layer Perceptron + Softmax layer as the
tag decoder layer. The architecture can be a bidirectional LSTM with a softmax layer on top. The softmax layer predicts the tag for each token (e.g. B-PER, I_LOC). One shortcoming of using the softmax layer on top is that each token is predicted indepently. So it is possible that the sequence of tags is incosistent, for exaple, B_PER, I_LOC, O, O, I_PER.
In order to overcome this shortcoming, instead of using a softmax layer, we can use a Conditional Random Field (CRF) layer on top. When CRF is predicting the tag for a sequence, it also considers the surronding tokens and their tags into account as well. More formally, a Conditional Random Field* (CRF) is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs.


