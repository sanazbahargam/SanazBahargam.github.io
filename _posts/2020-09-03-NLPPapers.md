---
title: 'NLP Papaers'
date: 2020-09-03
permalink: /posts/2020/09/NLPPapers/
tags:
  - NLP
  - Transformers
---


[Pre-training via Paraphrasing](https://arxiv.org/abs/2006.15020)

Authors: Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, Luke Zettlemoyer

Facebook


XLNet, [Generalized Autoregressive Pretraining for Language Understanding‚Äù from Carnegie Mellon and Google Research](https://arxiv.org/abs/1906.08237)  

Authors: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le

Year: June 2019


[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova

CMU, Google

Year: May 2019

[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

Authors: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever 

OpenAI

Year: June 2018

[Attention is all you need](https://arxiv.org/abs/1706.03762) 

Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin

Google

Year: Dec 2017
