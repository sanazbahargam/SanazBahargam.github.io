---
title: 'NLP Papaers'
date: 2020-09-03
permalink: /posts/2020/09/NLPPapers/
tags:
  - NLP
  - Transformers
---

[AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization](https://arxiv.org/abs/2008.11869)

Authors: Xinsong Zhang, Hang Li

ByteDance AI Lab

Year: August 2020

[Pre-training via Paraphrasing](https://arxiv.org/abs/2006.15020)

Authors: Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, Luke Zettlemoyer

Facebook

Year: June 2020


[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)

Authors:Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer

Facebook

Year: October 2019

[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)

Authors: Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut

Google and Toyota Technological Institute

Year: September 2019


[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)

Auhtors: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov

UW and Facebook 

Year: July 2019

[Generalized Autoregressive Pretraining for Language Understanding‚Äù from Carnegie Mellon and Google Research](https://arxiv.org/abs/1906.08237), XLNet

Authors: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le

Year: June 2019


[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova

CMU, Google

Year: May 2019

[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

Authors: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever 

OpenAI

Year: June 2018

[Attention is all you need](https://arxiv.org/abs/1706.03762) 

Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin

Google

Year: Dec 2017
