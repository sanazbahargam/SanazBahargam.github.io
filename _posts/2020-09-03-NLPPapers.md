---
title: 'NLP Papaers'
date: 2020-09-03
permalink: /posts/2020/09/NLPPapers/
tags:
  - NLP
  - Transformers
---

[AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization](https://arxiv.org/abs/2008.11869)

Authors: Xinsong Zhang, Hang Li

ByteDance AI Lab

Year: August 2020


[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.](https://arxiv.org/pdf/1910.10683.pdf) T5

Authors: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu

Google

Year: July 2020

[Pre-training via Paraphrasing](https://arxiv.org/abs/2006.15020)

Authors: Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, Luke Zettlemoyer

Facebook

Year: June 2020



[ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.](https://arxiv.org/abs/2003.10555)

Authors: Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.

Google and Stanford

Year: March 2020



[Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172)

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis

Facebook and Stanford [Presenation in ACL 2020, "Beyond BERT" by Mike Lewis](https://slideslive.com/38929793/beyond-bert)

Year: Feb 2020



[ConveRT: Efficient and Accurate Conversational Representations from Transformers](https://arxiv.org/abs/1911.03688)

Authors: Matthew Henderson, Iñigo Casanueva, Nikola Mrkšić, Pei-Hao Su, Tsung-Hsien Wen, Ivan Vulić

PolyAI 

Year: Nov 2019

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)

Authors:Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer

Facebook

Year: October 2019

[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)

Authors: Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut

Google and Toyota Technological Institute

Year: September 2019


[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)

Auhtors: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov

UW and Facebook 

Year: July 2019

[Generalized Autoregressive Pretraining for Language Understanding” from Carnegie Mellon and Google Research](https://arxiv.org/abs/1906.08237), XLNet

Authors: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le

Year: June 2019


[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova

CMU, Google

Year: May 2019



[Cross-lingual Language Model Pretraining](https://arxiv.org/pdf/1901.07291.pdf)

Authors:Guillaume Lample, Alexis Conneau

Facebook

year: January 2019

[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

Authors: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever 

OpenAI

Year: June 2018


[Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf) ELMo

Auhtors: Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee and Luke Zettlemoyer

Allen Institute for Artificial Intelligence and UW

year: March 2018

[Attention is all you need](https://arxiv.org/abs/1706.03762) 

Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin

Google

Year: Dec 2017
